#!/usr/bin/env python3# knowledge_distillation.py"""Module for performing knowledge distillation with YOLOv8 models.Using a teacher-student approach where a larger model (teacher) guidesthe training of a smaller model (student).Updated for compatibility with newer Ultralytics versions."""import osimport yamlimport jsonimport timeimport torchimport argparseimport numpy as npfrom pathlib import Pathfrom ultralytics import YOLOfrom ultralytics.utils import opsfrom ultralytics.engine.trainer import BaseTrainerfrom ultralytics.utils.torch_utils import de_parallelfrom ultralytics.cfg import get_cfgclass YOLODistillationTrainer(BaseTrainer):    """    Custom YOLO trainer that implements knowledge distillation    from a teacher model to a student model.    """    def __init__(self, teacher_model, student_model, cfg, distill_temp=5.0, distill_weight=0.7, feat_distill_weight=0.2):        """Initialize the distillation trainer with both teacher and student models."""        # Initialize the BaseTrainer with the configuration        super().__init__(cfg=cfg, overrides={})                # Set student model manually        self.model = student_model                # Store teacher model        self.teacher_model = teacher_model        self.teacher_model.eval()  # Set teacher to evaluation mode                # Store distillation parameters        self.distill_temp = distill_temp  # Temperature for softening predictions (higher for detection)        self.distill_weight = distill_weight  # Weight for distillation loss (0-1)        self.feat_distill_weight = feat_distill_weight  # Weight for feature distillation                # Calculate remaining weight for original task loss        self.task_weight = max(0, 1.0 - self.distill_weight - self.feat_distill_weight)                # Set up KL divergence loss for soft targets        self.kl_loss = torch.nn.KLDivLoss(reduction='batchmean')                # Get number of classes from student model        self.nc = student_model.nc if hasattr(student_model, 'nc') else 80  # Default to COCO                # Create adaptation layers if teacher and student architectures differ        self.adaptation_layers = self._create_adaptation_layers(teacher_model, student_model)                # Enable gradient checkpointing for memory optimization        self.use_checkpointing = False        if hasattr(student_model, 'gradient_checkpointing'):            self.use_checkpointing = student_model.gradient_checkpointing                print(f"Distillation: temperature={distill_temp}, weight={distill_weight}, feature weight={self.feat_distill_weight}, task weight={self.task_weight}")                # Handle distributed training setup        self.is_distributed = False        if torch.cuda.device_count() > 1 and self.device != 'cpu':            self.is_distributed = True            print(f"Using {torch.cuda.device_count()} GPUs for distillation")                    # Initialize metrics tracking        self.metrics = {}        self.distill_loss_val = 0.0        self.feat_loss = 0.0    def _create_adaptation_layers(self, teacher_model, student_model):        """Create adaptation layers to align teacher and student feature maps if needed."""        adaptation_layers = torch.nn.ModuleDict()                # Try to extract model architecture information (this depends on YOLOv8 version)        try:            # Get teacher and student feature dimensions            # This is a simplified approach - in practice, you would need to inspect            # the actual model architecture more carefully            teacher_channels = self._get_model_feature_channels(teacher_model)            student_channels = self._get_model_feature_channels(student_model)                        # Create adaptation layers for each feature level where dimensions differ            for level, (t_ch, s_ch) in enumerate(zip(teacher_channels, student_channels)):                if t_ch != s_ch:                    adaptation_layers[f'adapt_level_{level}'] = torch.nn.Conv2d(                        t_ch, s_ch, kernel_size=1, stride=1, padding=0, bias=False                    )                    print(f"Created adaptation layer for level {level}: {t_ch} -> {s_ch}")        except Exception as e:            print(f"Warning: Could not create adaptation layers - {str(e)}")            print("Will use basic feature alignment instead")                return adaptation_layers    def _get_model_feature_channels(self, model):        """Try to extract feature channels from model architecture."""        # This is simplified - real implementation depends on specific YOLOv8 architecture        try:            # Try to get backbone feature channel information            if hasattr(model, 'backbone'):                if hasattr(model.backbone, 'channels'):                    return model.backbone.channels                        # Fallback to default values (based on YOLOv8 architecture)            # These values need to be adjusted based on actual model being used            return [256, 512, 1024]  # Common feature dimensions in YOLO models        except:            return [256, 512, 1024]  # Default values    def preprocess_batch(self, batch):        """Preprocess batch before training."""        # Standard YOLO preprocessing        batch = super().preprocess_batch(batch)        return batch    def process_yolo_output(self, pred):        """Handle various YOLOv8 output formats"""        if isinstance(pred, (list, tuple)):  # Older YOLO versions            return pred[0] if len(pred) > 0 else pred  # Take the first output        elif isinstance(pred, dict):  # Some versions use dict            return pred.get('output', pred)  # Get 'output' key or return the dict        return pred  # Assume tensor output    def forward_with_checkpointing(self, model, x):        """Use gradient checkpointing to save memory if enabled"""        if self.use_checkpointing and hasattr(torch.utils.checkpoint, 'checkpoint'):            return torch.utils.checkpoint.checkpoint(model, x)        return model(x)    def get_teacher_outputs(self, imgs):        """Get teacher model outputs for the given images."""        with torch.no_grad():  # Don't compute gradients for teacher            # Get teacher predictions            teacher_preds = self.teacher_model(imgs)            return self.process_yolo_output(teacher_preds)    def get_feature_maps(self, model, imgs):        """        Extract intermediate feature maps from the model.        Uses direct access to backbone outputs when possible.        """        # Try direct backbone access first (most reliable)        if hasattr(model, 'backbone'):            try:                with torch.no_grad() if model is self.teacher_model else torch.enable_grad():                    # Direct access to backbone                    features = model.backbone(imgs)                    # Handle different return types                    if isinstance(features, dict):                        return list(features.values())                    elif isinstance(features, (list, tuple)):                        return features                    else:                        return [features]  # Convert single tensor to list            except Exception as e:                print(f"Direct backbone access failed: {e}, falling back to hooks")                # Fallback to hook-based approach        feature_maps = []                def hook_fn(module, input, output):            feature_maps.append(output)                # Try to register hooks on model layers        try:            hooks = []            # Target backbone layers that produce feature maps            if hasattr(model, 'backbone'):                for name, module in model.backbone.named_modules():                    # Target specific layers that produce the feature pyramid                    if isinstance(module, torch.nn.Conv2d) and 'cv' in name and any(f'm{i}' in name for i in [2, 4, 6]):                        hooks.append(module.register_forward_hook(hook_fn))                        # If no hooks were registered, fall back to a simpler approach            if not hooks:                # Register hooks on the last 3 modules that might produce feature maps                conv_modules = [m for m in model.modules() if isinstance(m, torch.nn.Conv2d)]                for module in conv_modules[-3:]:                    hooks.append(module.register_forward_hook(hook_fn))                        # Forward pass to collect feature maps            with torch.no_grad() if model is self.teacher_model else torch.enable_grad():                _ = model(imgs)                        # Remove hooks            for hook in hooks:                hook.remove()                except Exception as e:            print(f"Warning: Could not extract feature maps - {str(e)}")                return feature_maps    def feature_distillation_loss(self, student_features, teacher_features):        """Calculate feature distillation loss between teacher and student feature maps."""        if not student_features or not teacher_features:            return torch.tensor(0.0, device=self.device)                losses = []                # Match feature maps by selecting those with closest spatial dimensions        for s_feat in student_features:            best_match = None            best_match_diff = float('inf')                        # Find best matching teacher feature map            for t_feat in teacher_features:                # Simple matching by spatial dimensions                s_h, s_w = s_feat.shape[2:]                t_h, t_w = t_feat.shape[2:]                diff = abs(s_h - t_h) + abs(s_w - t_w)                                if diff < best_match_diff:                    best_match = t_feat                    best_match_diff = diff                        if best_match is not None:                # Get matched teacher feature                t_feat = best_match                                # Check if we need adaptation                if t_feat.shape[1] != s_feat.shape[1]:                    # Channel dimension mismatch, try to adapt                    level = len(losses)  # Use current loss count as level index                    adapt_key = f'adapt_level_{level}'                                        if adapt_key in self.adaptation_layers:                        # Use adaptation layer                        t_feat = self.adaptation_layers[adapt_key](t_feat)                    else:                        # Simple interpolation (not ideal)                        t_feat = torch.nn.functional.interpolate(                            t_feat,                             size=s_feat.shape[1:],                            mode='bilinear',                             align_corners=False                        )                                # Ensure spatial dimensions match                if t_feat.shape[2:] != s_feat.shape[2:]:                    t_feat = torch.nn.functional.interpolate(                        t_feat,                         size=s_feat.shape[2:],                        mode='bilinear',                         align_corners=False                    )                                # Calculate MSE loss between feature maps                loss = torch.nn.functional.mse_loss(s_feat, t_feat)                losses.append(loss)                # Average losses across all feature maps        if losses:            return torch.stack(losses).mean()        else:            return torch.tensor(0.0, device=self.device)    def distillation_loss(self, student_preds, teacher_preds):        """        Calculate distillation loss between teacher and student predictions.        Handles the complex YOLO output format separately for box, objectness, and class predictions.        """        total_loss = 0.0        F = torch.nn.functional                # Process each detection head        for idx, (s_pred, t_pred) in enumerate(zip(student_preds, teacher_preds)):            try:                # YOLOv8 format can vary - need to handle different output structures                if isinstance(s_pred, list) and len(s_pred) > 0:                    # Handle list-type outputs                    s_box, s_cls = s_pred[0], s_pred[1]                    t_box, t_cls = t_pred[0], t_pred[1]                                        # Classification distillation                    s_log_probs = F.log_softmax(s_cls / self.distill_temp, dim=-1)                    t_probs = F.softmax(t_cls / self.distill_temp, dim=-1)                    cls_loss = self.kl_loss(s_log_probs, t_probs) * (self.distill_temp ** 2)                                        # Box regression distillation (MSE)                    box_loss = F.mse_loss(s_box, t_box)                                        # Combined loss for this head                    head_loss = 0.5 * cls_loss + 0.5 * box_loss                                    elif isinstance(s_pred, torch.Tensor) and s_pred.dim() >= 2:                    # Handle tensor outputs - this is the more common format in YOLOv8                    # Try to split predictions into components (box, obj, cls)                    try:                        # YOLO output format: [x, y, w, h, obj, cls1, cls2, ...]                        box_channels = 4  # x, y, w, h                        obj_channels = 1  # objectness                        cls_channels = self.nc  # class predictions                                                # Split predictions                        if s_pred.shape[-1] >= box_channels + obj_channels + cls_channels:                            # Standard output format with box, obj, cls                            s_box, s_obj, s_cls = torch.split(                                s_pred,                                 [box_channels, obj_channels, cls_channels],                                 dim=-1                            )                            t_box, t_obj, t_cls = torch.split(                                t_pred,                                 [box_channels, obj_channels, cls_channels],                                 dim=-1                            )                                                        # Classification distillation with temperature                            s_log_probs = F.log_softmax(s_cls / self.distill_temp, dim=-1)                            t_probs = F.softmax(t_cls / self.distill_temp, dim=-1)                            cls_loss = self.kl_loss(s_log_probs, t_probs) * (self.distill_temp ** 2)                                                        # Box regression distillation (MSE)                            box_loss = F.mse_loss(s_box, t_box)                                                        # Objectness distillation (BCE)                            obj_loss = F.binary_cross_entropy_with_logits(s_obj, t_obj.sigmoid())                                                        # Weighted sum for this head                            head_loss = 0.3 * cls_loss + 0.5 * box_loss + 0.2 * obj_loss                        else:                            # Simplified fallback if shapes don't match expectations                            head_loss = F.mse_loss(s_pred, t_pred)                                                except Exception as e:                        print(f"Warning in distillation loss: {e}")                        # Fallback to simple MSE loss                        head_loss = F.mse_loss(s_pred, t_pred)                else:                    # Fallback for other output types                    head_loss = F.mse_loss(s_pred, t_pred)                                    total_loss += head_loss                            except Exception as e:                print(f"Error processing head {idx}: {e}")                continue                # Average across all detection heads        if len(student_preds) > 0:            total_loss = total_loss / len(student_preds)                return total_loss    def _do_train(self, world_size=1):        """        Training loop that includes knowledge distillation.        """        # Standard YOLO training initialization        torch.cuda.empty_cache()        self.epoch_time = None        self.epoch_time_start = time.time()        self.train_time_start = time.time()        nb = len(self.train_loader)        nw = max(round(self.args.warmup_epochs * nb), 100) if self.args.warmup_epochs > 0 else -1        last_opt_step = -1        self.run_callbacks('on_train_start')                # Main training loop        for epoch in range(self.start_epoch, self.epochs):            self.epoch = epoch            self.run_callbacks('on_train_epoch_start')            self.model.train()                        pbar = self.progress_bar(self.train_loader, description=f"Epoch {epoch}/{self.epochs}")            self.tloss = None                        # Batch loop            for i, batch in enumerate(pbar):                self.run_callbacks('on_train_batch_start')                # Warmup                ni = i + nb * epoch                if ni <= nw:                    xi = [0, nw]  # x interp                    self.accumulate = max(1, np.interp(ni, xi, [1, self.args.nbs / self.batch_size]).round())                    for j, x in enumerate(self.optimizer.param_groups):                        # Bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0                        x['lr'] = np.interp(ni, xi, [0.1 if j == 0 else 0.0, x['initial_lr'] * self.lf(epoch)])                        if 'momentum' in x:                            x['momentum'] = np.interp(ni, xi, [0.9, self.args.momentum])                                # Forward pass                batch = self.preprocess_batch(batch)                imgs = batch['img']                                # Get teacher predictions and feature maps (without gradient)                teacher_preds = self.get_teacher_outputs(imgs)                teacher_features = self.get_feature_maps(self.teacher_model, imgs)                                # Get student predictions and feature maps (with gradient)                if self.use_checkpointing:                    student_preds = self.forward_with_checkpointing(self.model, imgs)                else:                    student_preds = self.model(imgs)                student_features = self.get_feature_maps(self.model, imgs)                                # Process outputs to handle different formats                student_preds = self.process_yolo_output(student_preds)                teacher_preds = self.process_yolo_output(teacher_preds)                                # Calculate standard YOLO detection loss                yolo_loss, student_loss_items = self.criterion(student_preds, batch)                                # Initialize total loss with task loss weighted by task_weight                loss = self.task_weight * yolo_loss                                # Calculate distillation losses                total_distill_loss = 0                if self.distill_weight > 0:                    # Output distillation loss                    distill_loss = self.distillation_loss(student_preds, teacher_preds)                    loss += self.distill_weight * distill_loss                    total_distill_loss += distill_loss.item()                                        # Feature distillation loss (if features were extracted successfully)                    if teacher_features and student_features and self.feat_distill_weight > 0:                        feat_loss = self.feature_distillation_loss(student_features, teacher_features)                        loss += self.feat_distill_weight * feat_loss                                                # Track feature distillation loss for logging                        if not hasattr(self, 'feat_loss'):                            self.feat_loss = feat_loss.item()                        else:                            self.feat_loss = 0.9 * self.feat_loss + 0.1 * feat_loss.item()                                # Calculate total loss                if self.args.rank != -1:                    loss *= world_size                                    # Backward pass                self.scaler.scale(loss).backward()                                # Optimize                if ni - last_opt_step >= self.accumulate:                    self.scaler.unscale_(self.optimizer)                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=10.0)                    self.scaler.step(self.optimizer)                    self.scaler.update()                    self.optimizer.zero_grad()                    if self.ema:                        self.ema.update(self.model)                    last_opt_step = ni                                # Loss items                if self.tloss is None:                    self.tloss = student_loss_items                    # Track distillation loss separately                    self.distill_loss_val = total_distill_loss                else:                    self.tloss = [(1 - 0.9) * l + 0.9 * nl for l, nl in zip(self.tloss, student_loss_items)]                    # Update distillation loss tracking                    self.distill_loss_val = 0.9 * self.distill_loss_val + 0.1 * total_distill_loss                                # Progress bar with added distillation loss info                mem = f"{torch.cuda.memory_reserved() / 1E9:.3g}G"                loss_len = len(student_loss_items)                losses = [round(float(x), 5) for x in student_loss_items[:loss_len]]                                # Add distillation losses to progress description if enabled                if self.distill_weight > 0:                    distill_info = f" D:{self.distill_loss_val:.4g}"                    if hasattr(self, 'feat_loss'):                        distill_info += f" F:{self.feat_loss:.4g}"                else:                    distill_info = ""                                pbar.set_description(                    ("%11s" * 2 + "%11.4g" * (2 + loss_len) + "%s") %                    (f"{epoch}/{self.epochs - 1}", mem, *losses, batch["cls"].shape[0], batch["img"].shape[-1], distill_info))                                self.run_callbacks('on_train_batch_end')                                if self.args.sync_bn and ni in {3, 5, 7}:                    torch.cuda.empty_cache()                        # End epoch            self.lr = {f"lr/pg{ir}": x["lr"] for ir, x in enumerate(self.optimizer.param_groups)}  # Learning rates                        # Track distillation metrics            if self.distill_weight > 0:                self.metrics['distill_loss'] = self.distill_loss_val                if hasattr(self, 'feat_loss'):                    self.metrics['feat_distill_loss'] = self.feat_loss                        self.scheduler.step()            self.run_callbacks('on_train_epoch_end')                        # Validation            if self.args.val:                self.metrics, self.fitness = self.validate()            self.save_metrics(metrics={**self.label_loss_items(self.tloss), **self.metrics, **self.lr})            self.stop = self.should_stop()            self.run_callbacks('on_fit_epoch_end')            torch.cuda.empty_cache()                        # Early stopping            if self.stop:                break                self.run_callbacks('on_train_end')        torch.cuda.empty_cache()        return self.metricsdef distill_knowledge(student_model_path, teacher_model_path, data_yaml,                      output_dir, epochs=100, batch_size=32, device='0',                     distillation_temp=5.0, distillation_weight=0.7, feat_distill_weight=0.2):    """    Perform knowledge distillation from teacher to student model.        Args:        student_model_path: Path to student model weights (.pt file)        teacher_model_path: Path to teacher model weights (.pt file)        data_yaml: Path to data YAML file        output_dir: Directory to save distilled model        epochs: Number of epochs to train (100+ recommended for distillation)        batch_size: Batch size for training (32+ recommended)        device: Device to use for training ('0' for GPU 0, 'cpu' for CPU)        distillation_temp: Temperature for softening the teacher's predictions (higher for detection tasks)        distillation_weight: Weight for the distillation loss (0-1)        feat_distill_weight: Weight for feature distillation loss (0-1)            Returns:        dict: Performance metrics of the distilled model    """    print(f"Starting knowledge distillation from {teacher_model_path} to {student_model_path}")        # Create output directory    os.makedirs(output_dir, exist_ok=True)        # Check if CUDA is available when GPU is requested    if device != 'cpu' and not torch.cuda.is_available():        print("CUDA not available. Falling back to CPU.")        device = 'cpu'        # Load teacher model    try:        teacher = YOLO(teacher_model_path)        print(f"Teacher model loaded: {teacher_model_path}")    except Exception as e:        print(f"Error loading teacher model: {str(e)}")        return None        # Load student model    try:        student = YOLO(student_model_path)        print(f"Student model loaded: {student_model_path}")    except Exception as e:        print(f"Error loading student model: {str(e)}")        return None        # Handle distributed training if multiple GPUs are available    if torch.cuda.device_count() > 1 and device != 'cpu':        print(f"Using {torch.cuda.device_count()} GPUs for distillation!")        teacher.model = torch.nn.DataParallel(teacher.model)        student.model = torch.nn.DataParallel(student.model)        # Create a custom configuration for distillation    distill_config = {        'model': student_model_path,        'data': data_yaml,        'epochs': epochs,        'batch': batch_size,        'device': device,        'name': 'distilled_model',        'project': output_dir,        # Custom distillation parameters        'teacher_model': teacher_model_path,        'distill_temp': distillation_temp,        'distill_weight': distillation_weight,        'feat_distill_weight': feat_distill_weight,        # Recommended for distillation        'lr0': 0.001,  # Lower initial learning rate (1/3 of standard)        'lrf': 0.01,   # Final learning rate factor        'momentum': 0.937,        'weight_decay': 0.0005,        'warmup_epochs': 5.0,  # Longer warmup period        'patience': 50,  # More patience for early stopping    }        # Save custom configuration    config_path = os.path.join(output_dir, 'distill_config.yaml')    with open(config_path, 'w') as f:        yaml.dump(distill_config, f, sort_keys=False)        print(f"Distillation configuration saved to {config_path}")        try:        # Setup proper training configuration        overrides = {            'model': student_model_path,            'data': data_yaml,            'epochs': epochs,            'batch': batch_size,            'device': device,            'project': output_dir,            'name': 'distilled_model',            'lr0': 0.001,  # Lower learning rate for distillation            'lrf': 0.01,   # Final LR factor            'patience': 50,  # More patience for distillation            'warmup_epochs': 5.0,            'momentum': 0.937,            'weight_decay': 0.0005,            'save': True,            'exist_ok': True,            'verbose': True,        }                # Save original student model for later comparison        original_student_path = os.path.join(output_dir, 'original_student.pt')        torch.save(student.model.state_dict(), original_student_path)                # Get base configuration for the student model        if hasattr(student, 'ckpt'):            # For newer Ultralytics versions            cfg = get_cfg(model=student_model_path)        else:            # For older versions, create default cfg            cfg = get_cfg("yolov8n.yaml")                # Update cfg with our overrides        for k, v in overrides.items():            setattr(cfg, k, v)                # Create custom distillation trainer        distill_trainer = YOLODistillationTrainer(            teacher_model=teacher.model,            student_model=student.model,            cfg=cfg,            distill_temp=distillation_temp,            distill_weight=distillation_weight,            feat_distill_weight=feat_distill_weight        )                # Replace student's trainer with our distillation trainer        student.trainer = distill_trainer                print(f"Starting distillation training for {epochs} epochs...")                # Start training with distillation        student.train()            except Exception as e:        print(f"Error during distillation training: {str(e)}")        import traceback        traceback.print_exc()        print("Falling back to standard training...")                # Fallback to standard training if custom distillation fails        student = YOLO(student_model_path)  # Reload student model        student.train(            data=data_yaml,            epochs=epochs,            batch=batch_size,            device=device,            name='distilled_model',            project=output_dir        )        # Save the distilled model    distilled_model_path = os.path.join(output_dir, 'distilled_model', 'weights', 'best.pt')        # Validate the distilled model    if os.path.exists(distilled_model_path):        print(f"Validating distilled model: {distilled_model_path}")        # Load the final model for validation        final_model = YOLO(distilled_model_path)        val_results = final_model.val(data=data_yaml)                # Extract metrics        metrics = {            "model_name": "distilled_" + Path(student_model_path).stem,            "student_model": str(student_model_path),            "teacher_model": str(teacher_model_path),            "distillation_temp": distillation_temp,            "distillation_weight": distillation_weight,            "feat_distill_weight": feat_distill_weight,            "map50": float(val_results.box.map50),            "map50_95": float(val_results.box.map),            "precision": float(val_results.box.mp),            "recall": float(val_results.box.mr)        }                # Add derived metrics        if hasattr(val_results.box, 'f1'):            metrics["f1"] = float(val_results.box.f1)        else:            # Calculate F1 manually if not provided            precision = metrics["precision"]            recall = metrics["recall"]            if precision + recall > 0:                metrics["f1"] = 2 * precision * recall / (precision + recall)            else:                metrics["f1"] = 0.0                # Calculate speed metrics        try:            print("Measuring inference speed...")            # Simple speed test on CPU and GPU (if available)            inference_times = []                        # Test on device            start_time = time.time()            for _ in range(20):  # 20 forward passes                _ = final_model.predict(torch.zeros((1, 3, 640, 640)).to(device))            device_inference_time = (time.time() - start_time) / 20            metrics["inference_time_ms"] = device_inference_time * 1000            metrics["fps"] = 1.0 / device_inference_time if device_inference_time > 0 else 0                        # Get model size            model_size_mb = os.path.getsize(distilled_model_path) / (1024 * 1024)            metrics["model_size_mb"] = model_size_mb        except Exception as e:            print(f"Error measuring speed: {e}")                # Compare with original student model        try:            print("Comparing with original student model...")            original_student = YOLO(student_model_path)            original_results = original_student.val(data=data_yaml)                        # Calculate improvements            improvement = {                "map50": float(val_results.box.map50) - float(original_results.box.map50),                "map50_95": float(val_results.box.map) - float(original_results.box.map),                "precision": float(val_results.box.mp) - float(original_results.box.mp),                "recall": float(val_results.box.mr) - float(original_results.box.mr)            }                        metrics["improvements"] = improvement                        # Log improvements            print(f"Improvements after distillation:")            print(f"  mAP@0.5: {improvement['map50']:.4f}")            print(f"  mAP@0.5:0.95: {improvement['map50_95']:.4f}")            print(f"  Precision: {improvement['precision']:.4f}")            print(f"  Recall: {improvement['recall']:.4f}")        except Exception as e:            print(f"Error comparing with original model: {e}")                # Save metrics        metrics_path = os.path.join(output_dir, 'distilled_model', 'distillation_metrics.json')        with open(metrics_path, 'w') as f:            json.dump(metrics, f, indent=4)                print(f"Distillation completed. Results saved to {output_dir}")        return metrics    else:        print(f"Error: Distilled model not found at {distilled_model_path}")        return None
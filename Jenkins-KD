pipeline {
    agent any

    parameters {
        string(name: 'MODEL_NAME', defaultValue: 'yolov8_kd_model', description: 'Knowledge distilled model name')
        string(name: 'GDRIVE_FILE_ID', defaultValue: '1R44tNwMYBU3kaQLB2cgqzb8HMNisEuqA', description: 'Google Drive file ID for dataset')
        choice(name: 'TEACHER_MODEL', choices: ['yolov8x'], description: 'Teacher model for knowledge distillation')
        choice(name: 'STUDENT_MODEL', choices: ['yolov8n'], description: 'Student model for knowledge distillation')
        choice(name: 'KD_EPOCHS', choices: ['10', '30', '50', '100'], description: 'Number of epochs for knowledge distillation')
        booleanParam(name: 'SKIP_DOWNLOAD', defaultValue: false, description: 'Skip dataset download if already available')
        booleanParam(name: 'SKIP_AUGMENTATION', defaultValue: false, description: 'Skip dataset augmentation')
        booleanParam(name: 'CLEAN_WORKSPACE', defaultValue: true, description: 'Clean workspace after build')
    }

    environment {
        // Dataset related variables
        OUTPUT_ZIP = 'dataset.zip'
        EXTRACT_DIR = 'extracted_dataset'
        DATA_YAML_PATH_FILE = 'data_yaml_path.txt'

        // Virtual environment and repository
        VENV_NAME = 'kd_venv'
        KD_REPO = 'yolov8-kd'

        // Paths for generated files
        ARTIFACT_DIR = "${WORKSPACE}/artifacts"
        KD_RESULT_DIR = "${WORKSPACE}/kd_results"
        AUGMENTATION_PATH_FILE = "${WORKSPACE}/augmentation_dir_path.txt"

        // S3 bucket information (if needed)
        S3_BUCKET = 'yolov8-model-repository'
        S3_PREFIX = 'yolov8_kd_models'
    }

    options {
        // Set timeout for the entire build
        timeout(time: 8, unit: 'HOURS')
        // Keep the 10 most recent builds
        buildDiscarder(logRotator(numToKeepStr: '10'))
        // Don't run concurrent builds to avoid conflicts
        disableConcurrentBuilds()
        // Add timestamps to console output
        timestamps()
    }

    stages {
        stage('Initialize Workspace') {
            steps {
                script {
                    // Create directories for outputs
                    sh '''
                        mkdir -p ${ARTIFACT_DIR}
                        mkdir -p ${KD_RESULT_DIR}
                        mkdir -p email_artifacts
                        mkdir -p downloads
                    '''

                    // Display build information
                    echo "Starting YOLOv8 Knowledge Distillation Pipeline"
                    echo "Model Name: ${params.MODEL_NAME}"
                    echo "Teacher Model: ${params.TEACHER_MODEL}"
                    echo "Student Model: ${params.STUDENT_MODEL}"
                    echo "KD Epochs: ${params.KD_EPOCHS}"
                }
            }
        }

        stage('Setup Virtual Environment') {
            steps {
                sh '''#!/bin/bash -e
                    # Create virtual environment if it doesn't exist
                    if [ ! -d "${VENV_NAME}" ]; then
                        echo "Creating virtual environment..."
                        python3 -m venv ${VENV_NAME}
                    else
                        echo "Virtual environment already exists."
                    fi

                    # Activate virtual environment and upgrade pip
                    source ${VENV_NAME}/bin/activate
                    python -m pip install --upgrade pip

                    # Install core dependencies
                    pip install torch torchvision ultralytics matplotlib pandas seaborn numpy tqdm PyYAML

                    echo "Virtual environment setup complete."
                '''
            }
        }

        stage('Download Dataset') {
            when {
                expression { !params.SKIP_DOWNLOAD }
            }
            steps {
                sh '''#!/bin/bash -e
                    # Activate virtual environment
                    source ${VENV_NAME}/bin/activate

                    # Check if dataset already exists
                    if [ -f "downloads/${OUTPUT_ZIP}" ] && [ -d "downloads/${EXTRACT_DIR}" ]; then
                        echo "Dataset already exists. Skipping download."
                        exit 0
                    fi

                    # Install gdown for Google Drive downloads
                    echo "Installing gdown..."
                    pip install gdown

                    # Download the file from Google Drive using gdown
                    echo "Downloading dataset from Google Drive..."
                    cd downloads
                    python -m gdown https://drive.google.com/uc?id=${GDRIVE_FILE_ID} -O ${OUTPUT_ZIP}

                    # Check if download was successful and file has content
                    if [ ! -f "${OUTPUT_ZIP}" ]; then
                        echo "Download failed! File not found."
                        exit 1
                    fi

                    # Verify file size
                    FILE_SIZE=$(stat -c%s "${OUTPUT_ZIP}")
                    if [ $FILE_SIZE -lt 10000 ]; then
                        echo "Download appears incomplete. File size is only $FILE_SIZE bytes."
                        echo "Content of the file:"
                        head -c 1000 "${OUTPUT_ZIP}"
                        exit 1
                    fi

                    echo "Download completed successfully. File size: $FILE_SIZE bytes"
                '''

                // Archive the dataset as an artifact
                archiveArtifacts artifacts: "downloads/dataset.zip", fingerprint: true, allowEmptyArchive: true
            }
        }

        stage('Extract and Validate Dataset') {
            when {
                expression { !params.SKIP_DOWNLOAD }
            }
            steps {
                sh '''#!/bin/bash -e
                    # Move to downloads directory
                    cd downloads

                    # Create extraction directory if it doesn't exist
                    mkdir -p ${EXTRACT_DIR}

                    # Extract the zip file
                    echo "Extracting dataset..."
                    unzip -o ${OUTPUT_ZIP} -d ${EXTRACT_DIR}

                    # Find data.yaml file in the extracted directory
                    echo "Locating data.yaml file..."
                    DATA_YAML_PATH=$(find ${EXTRACT_DIR} -name "data.yaml" -type f | head -n 1)

                    if [ -z "${DATA_YAML_PATH}" ]; then
                        echo "data.yaml not found in the extracted dataset!"
                        echo "Contents of extraction directory:"
                        ls -la ${EXTRACT_DIR}
                        exit 1
                    fi

                    # Get absolute path
                    ABSOLUTE_PATH=$(readlink -f "${DATA_YAML_PATH}")

                    # Save the path to a file
                    echo "${ABSOLUTE_PATH}" > ${DATA_YAML_PATH_FILE}

                    echo "data.yaml path saved: ${ABSOLUTE_PATH}"

                    # Display first few lines of data.yaml
                    echo "First 5 lines of data.yaml:"
                    head -n 5 "${ABSOLUTE_PATH}"
                '''

                // Archive the path file as an artifact
                archiveArtifacts artifacts: 'downloads/data_yaml_path.txt', fingerprint: true, allowEmptyArchive: true
            }
        }

        stage('Setup Repository') {
            steps {
                sh '''#!/bin/bash -e
                    # Activate virtual environment
                    source ${VENV_NAME}/bin/activate

                    # Clone the knowledge distillation repository or create one if it doesn't exist
                    if [ ! -d "${KD_REPO}" ]; then
                        echo "Creating knowledge distillation directory..."
                        mkdir -p ${KD_REPO}

                        # Download knowledge_distillation.py script
                        echo "Creating knowledge distillation script..."
                        cat > ${KD_REPO}/knowledge_distillation.py << 'EOL'
import torch
import torch.nn.functional as F
import os
import argparse
import logging
import sys
from ultralytics import YOLO
from ultralytics.models.yolo.detect import DetectionTrainer
from ultralytics.utils import DEFAULT_CFG

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('kd_training.log')
    ]
)
logger = logging.getLogger('kd_trainer')

class KDTrainer(DetectionTrainer):
    def __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None, teacher_weights=None):
        if overrides is None:
            overrides = {}
        super().__init__(cfg=cfg, overrides=overrides, _callbacks=_callbacks)

        # Device handling
        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

        # Load teacher model
        self.teacher = YOLO(teacher_weights).model
        self.teacher.eval().to(self.device)
        for param in self.teacher.parameters():
            param.requires_grad = False  # Freeze teacher

        logger.info(f"Loaded teacher model from: {teacher_weights}")

        # KD hyperparameters
        self.alpha = float(overrides.get('alpha', 0.5))  # Weight for hard loss
        self.temperature = float(overrides.get('temperature', 2.0))  # Temperature for softening
        logger.info(f"KD parameters - Alpha: {self.alpha}, Temperature: {self.temperature}")

    def get_distillation_loss(self, student_outputs, teacher_outputs, batch):
        # Standard YOLO detection loss
        hard_loss = self.criterion(student_outputs, batch)

        # Initialize soft loss
        soft_loss = 0
        valid_outputs = 0

        # Handle single output case
        student_outputs = [student_outputs] if not isinstance(student_outputs, list) else student_outputs
        teacher_outputs = [teacher_outputs] if not isinstance(teacher_outputs, list) else teacher_outputs

        # Process each detection head
        for s_out, t_out in zip(student_outputs, teacher_outputs):
            if s_out.shape != t_out.shape:
                continue

            # Extract class predictions (YOLOv8 uses index 4 for class start)
            s_cls = s_out[..., 4:]  # Shape: [batch, anchors, classes]
            t_cls = t_out[..., 4:]

            # Compute KL divergence with numerical stability
            s_log_softmax = F.log_softmax(s_cls / self.temperature, dim=-1)
            t_softmax = F.softmax(t_cls / self.temperature, dim=-1).detach()

            # Clamp to avoid NaN
            t_softmax = torch.clamp(t_softmax, min=1e-7, max=1.0)

            soft_loss += F.kl_div(
                s_log_softmax,
                t_softmax,
                reduction='batchmean'
            ) * (self.temperature ** 2)
            valid_outputs += 1

        # Combine losses
        if valid_outputs == 0:
            return hard_loss  # Fallback if no matching outputs

        return self.alpha * hard_loss + (1 - self.alpha) * (soft_loss / valid_outputs)

    def training_step(self, batch):
        # Move batch to device if needed
        if batch['img'].device != self.device:
            batch['img'] = batch['img'].to(self.device)

        # Teacher forward pass (no gradients)
        with torch.no_grad():
            teacher_outputs = self.teacher(batch['img'])

        # Student forward pass
        student_outputs = self.model(batch['img'])

        # Compute combined loss
        loss = self.get_distillation_loss(student_outputs, teacher_outputs, batch)
        return loss

def train_student_model(cfg, teacher_weights):
    """Train the student model with knowledge distillation"""
    logger.info("="*50)
    logger.info("TRAINING STUDENT MODEL WITH KNOWLEDGE DISTILLATION")
    logger.info("="*50)

    # Initialize and train
    trainer = KDTrainer(overrides=cfg, teacher_weights=teacher_weights)
    results = trainer.train()

    return results

def main():
    parser = argparse.ArgumentParser(description='YOLOv8 Knowledge Distillation Training')

    # Basic configuration
    parser.add_argument('--data', type=str, required=True, help='Path to data.yaml')
    parser.add_argument('--student_model', type=str, default='yolov8n.yaml', help='Student model configuration')
    parser.add_argument('--teacher_model', type=str, default='yolov8x.yaml', help='Teacher model configuration')
    parser.add_argument('--teacher_weights', type=str, required=True, help='Path to pre-trained teacher weights')

    # Training parameters
    parser.add_argument('--epochs', type=int, default=50, help='Number of training epochs')
    parser.add_argument('--batch', type=int, default=16, help='Batch size')
    parser.add_argument('--imgsz', type=int, default=640, help='Image size')
    parser.add_argument('--device', type=str, default='0', help='Training device (GPU ID or cpu)')
    parser.add_argument('--workers', type=int, default=4, help='Number of dataloader workers')

    # Knowledge distillation parameters
    parser.add_argument('--alpha', type=float, default=0.5, help='Weight balance between hard and soft loss')
    parser.add_argument('--temperature', type=float, default=2.0, help='Softening temperature for KD')

    # Output naming
    parser.add_argument('--student_name', type=str, default='yolov8n_kd', help='Student experiment name')

    args = parser.parse_args()

    # Create configuration dictionary
    cfg = {
        'model': args.student_model,
        'data': args.data,
        'epochs': args.epochs,
        'imgsz': args.imgsz,
        'batch': args.batch,
        'device': args.device,
        'workers': args.workers,
        'name': args.student_name,
        'alpha': args.alpha,
        'temperature': args.temperature,
    }

    # Log configuration
    logger.info("Configuration:")
    for key, value in cfg.items():
        logger.info(f"  {key}: {value}")

    # Train student model
    teacher_weights = args.teacher_weights
    logger.info(f"Using pre-trained teacher weights: {teacher_weights}")

    results = train_student_model(cfg, teacher_weights)

    # Log results path
    results_path = os.path.join('runs/detect', args.student_name, 'results.csv')
    logger.info(f"Training completed. Results saved to: {results_path}")

    return 0

if __name__ == '__main__':
    try:
        exit_code = main()
        sys.exit(exit_code)
    except Exception as e:
        logger.exception("An error occurred during training")
        sys.exit(1)
EOL

                        echo "Creating analyze_results.py script..."
                        cat > ${KD_REPO}/analyze_results.py << 'EOL'
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
import json
import argparse
from pathlib import Path

def analyze_kd_results(results_file, output_dir):
    """Analyze knowledge distillation results and generate visualizations"""
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)

    # Read results
    df = pd.read_csv(results_file)

    # Print summary statistics
    print("\n== Knowledge Distillation Parameter Study Summary ==")
    print(f"Total experiments: {len(df)}")
    print("\nBest models by different metrics:")

    metrics = ['map50', 'map50_95', 'model_size_mb']
    best_rows = {}

    for metric in metrics:
        if metric == 'model_size_mb':
            # Lower is better for model size
            idx = df[metric].idxmin()
        else:
            # Higher is better for accuracy metrics
            idx = df[metric].idxmax()

        best_rows[metric] = df.loc[idx]

        if metric == 'model_size_mb':
            print(f"\nBest for {metric}: alpha={best_rows[metric]['alpha']}, "
                 f"temperature={best_rows[metric]['temperature']}, "
                 f"value={best_rows[metric][metric]:.4f} MB")
        else:
            print(f"\nBest for {metric}: alpha={best_rows[metric]['alpha']}, "
                 f"temperature={best_rows[metric]['temperature']}, "
                 f"value={best_rows[metric][metric]:.4f}")

    # Create normalized combined score (weighted average)
    print("\nCalculating combined score (weighted average of normalized metrics)...")

    # Normalize each metric to 0-1 range
    for metric in metrics:
        min_val = df[metric].min()
        max_val = df[metric].max()

        if max_val > min_val:
            if metric == 'model_size_mb':
                # For model size, smaller is better
                df[f'{metric}_norm'] = (max_val - df[metric]) / (max_val - min_val)
            else:
                # For accuracy metrics, larger is better
                df[f'{metric}_norm'] = (df[metric] - min_val) / (max_val - min_val)
        else:
            df[f'{metric}_norm'] = 0.5  # Default value if all models have same score

    # Create combined score (weighted)
    df['combined_score'] = (
        df['map50_norm'] * 0.4 +
        df['map50_95_norm'] * 0.4 +
        df['model_size_mb_norm'] * 0.2
    )

    # Find best overall model
    best_overall = df.loc[df['combined_score'].idxmax()]
    print(f"\nBest overall model (combined score):")
    print(f"  Alpha: {best_overall['alpha']}")
    print(f"  Temperature: {best_overall['temperature']}")
    print(f"  mAP@50: {best_overall['map50']:.4f}")
    print(f"  mAP@50-95: {best_overall['map50_95']:.4f}")
    print(f"  Model size: {best_overall['model_size_mb']:.2f} MB")
    print(f"  Combined score: {best_overall['combined_score']:.4f}")
    print(f"  Path: {best_overall['path']}")

    # Save best model info
    with open(os.path.join(output_dir, 'best_model.txt'), 'w') as f:
        f.write(f"Best overall model:\n")
        f.write(f"Alpha: {best_overall['alpha']}\n")
        f.write(f"Temperature: {best_overall['temperature']}\n")
        f.write(f"mAP@50: {best_overall['map50']:.4f}\n")
        f.write(f"mAP@50-95: {best_overall['map50_95']:.4f}\n")
        f.write(f"Model size: {best_overall['model_size_mb']:.2f} MB\n")
        f.write(f"Combined score: {best_overall['combined_score']:.4f}\n")
        f.write(f"Path: {best_overall['path']}\n")

    # Create visualizations
    print("\nGenerating visualizations...")

    # Create heatmaps for each metric
    for metric in metrics + ['combined_score']:
        plt.figure(figsize=(10, 8))
        pivot = df.pivot_table(values=metric, index='temperature', columns='alpha')

        if metric == 'model_size_mb':
            # For model size, use reverse colormap (smaller is better)
            cmap = 'viridis_r'
        else:
            # For other metrics, use regular colormap (larger is better)
            cmap = 'viridis'

        sns.heatmap(pivot, annot=True, fmt='.4f', cmap=cmap)
        plt.title(f'Impact of Alpha and Temperature on {metric}')
        plt.xlabel('Alpha (weight of hard loss)')
        plt.ylabel('Temperature')

        # Save the plot
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, f'{metric}_heatmap.png'))
        plt.close()

    # Create parameter correlation plot
    plt.figure(figsize=(10, 8))
    corr = df[metrics + ['alpha', 'temperature', 'combined_score']].corr()
    sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm')
    plt.title('Parameter Correlation')
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'parameter_correlation.png'))
    plt.close()

    # Save detailed report
    with open(os.path.join(output_dir, 'parameter_study_report.md'), 'w') as f:
        f.write('# Knowledge Distillation Parameter Study Report\n\n')

        f.write('## Parameter Combinations Tested\n\n')
        f.write('| Alpha | Temperature | mAP@50 | mAP@50-95 | Model Size (MB) | Combined Score |\n')
        f.write('|-------|-------------|--------|-----------|-----------------|---------------|\n')

        # Sort by combined score (descending)
        for _, row in df.sort_values('combined_score', ascending=False).iterrows():
            f.write(f"| {row['alpha']} | {row['temperature']} | {row['map50']:.4f} | {row['map50_95']:.4f} | {row['model_size_mb']:.2f} | {row['combined_score']:.4f} |\n")

        f.write('\n## Best Parameters\n\n')

        for metric in metrics:
            if metric == 'model_size_mb':
                best_row = df.loc[df[metric].idxmin()]
                f.write(f"* Best for {metric}: alpha={best_row['alpha']}, temperature={best_row['temperature']} (value={best_row[metric]:.4f} MB)\n")
            else:
                best_row = df.loc[df[metric].idxmax()]
                f.write(f"* Best for {metric}: alpha={best_row['alpha']}, temperature={best_row['temperature']} (value={best_row[metric]:.4f})\n")

        f.write(f"* Best overall (combined score): alpha={best_overall['alpha']}, temperature={best_overall['temperature']} (score={best_overall['combined_score']:.4f})\n\n")

        f.write('## Recommendations\n\n')
        f.write(f"Based on this parameter study, the recommended settings for knowledge distillation on this dataset are:\n\n")
        f.write(f"* **Alpha: {best_overall['alpha']}** - Controls the balance between hard loss (ground truth) and soft loss (teacher knowledge)\n")
        f.write(f"* **Temperature: {best_overall['temperature']}** - Controls the softening of probability distributions\n\n")
        f.write(f"These parameters provide the best balance between accuracy, speed, and model size.\n")

def main():
    parser = argparse.ArgumentParser(description="Analyze Knowledge Distillation Results")
    parser.add_argument("--results", type=str, required=True, help="Path to results CSV file")
    parser.add_argument("--output", type=str, default="analysis_results", help="Output directory for analysis results")

    args = parser.parse_args()
    analyze_kd_results(args.results, args.output)

if __name__ == "__main__":
    main()
EOL
                    else
                        echo "Knowledge distillation repository directory already exists, updating scripts..."
                    fi

                    # Install any additional requirements for the knowledge distillation
                    pip install ultralytics torch torchvision matplotlib pandas seaborn

                    echo "Knowledge distillation repository setup complete."
                '''
            }
        }

        stage('Dataset Augmentation') {
            when {
                expression { !params.SKIP_AUGMENTATION }
            }
            steps {
                sh '''#!/bin/bash -e
                    # Activate virtual environment
                    source ${VENV_NAME}/bin/activate

                    # Read the data.yaml path
                    DATA_YAML_PATH=$(cat downloads/data_yaml_path.txt)

                    # Get the dataset directory path (parent directory of data.yaml)
                    DATASET_DIR=$(dirname "${DATA_YAML_PATH}")

                    # Create augmented dataset directory
                    AUGMENTATION_DIR="${WORKSPACE}/augmented_dataset"
                    mkdir -p ${AUGMENTATION_DIR}

                    # Create a simple augmentation script
                    cat > ${KD_REPO}/simple_augmentation.py << 'EOL'
import os
import shutil
import random
import cv2
import numpy as np
import argparse
from pathlib import Path
from tqdm import tqdm
import yaml

def augment_image(img, bboxes, augment_type):
    h, w, _ = img.shape

    if augment_type == 'flip':
        # Horizontal flip
        img = cv2.flip(img, 1)
        # Adjust bounding boxes - x becomes 1-x
        for box in bboxes:
            box[1] = 1 - box[1]  # Flip x center

    elif augment_type == 'brightness':
        # Adjust brightness
        factor = random.uniform(0.5, 1.5)
        img = cv2.convertScaleAbs(img, alpha=factor, beta=0)

    elif augment_type == 'contrast':
        # Adjust contrast
        factor = random.uniform(0.5, 1.5)
        mean = np.mean(img)
        img = (img - mean) * factor + mean
        img = np.clip(img, 0, 255).astype(np.uint8)

    elif augment_type == 'rotate':
        # Small rotation (Â±15 degrees)
        angle = random.uniform(-15, 15)
        M = cv2.getRotationMatrix2D((w/2, h/2), angle, 1)
        img = cv2.warpAffine(img, M, (w, h))

        # Bounding box transformation is more complex
        # Simplified approach: keep the boxes unchanged for small rotations

    return img, bboxes

def augment_dataset(source_dir, output_dir, factor=2):
    # Create output directory structure
    os.makedirs(os.path.join(output_dir, 'images', 'train'), exist_ok=True)
    os.makedirs(os.path.join(output_dir, 'images', 'val'), exist_ok=True)
    os.makedirs(os.path.join(output_dir, 'labels', 'train'), exist_ok=True)
    os.makedirs(os.path.join(output_dir, 'labels', 'val'), exist_ok=True)

    # Read data.yaml
    data_yaml_path = os.path.join(source_dir, 'data.yaml')
    if os.path.exists(data_yaml_path):
        with open(data_yaml_path, 'r') as f:
            data_yaml = yaml.safe_load(f)
    else:
        # Try to find data.yaml in parent directories
        for parent in Path(source_dir).parents:
            data_yaml_path = os.path.join(parent, 'data.yaml')
            if os.path.exists(data_yaml_path):
                with open(data_yaml_path, 'r') as f:
                    data_yaml = yaml.safe_load(f)
                break
        else:
            # Create default data.yaml if not found
            data_yaml = {
                'nc': 0,  # Will be updated based on labels
                'names': [],
                'train': './images/train',
                'val': './images/val',
                'test': ''
            }

    # Find train/val image and label directories
    img_train_dir = os.path.join(source_dir, 'images', 'train')
    if not os.path.exists(img_train_dir):
        img_train_dir = os.path.join(source_dir, 'train', 'images')

    img_val_dir = os.path.join(source_dir, 'images', 'val')
    if not os.path.exists(img_val_dir):
        img_val_dir = os.path.join(source_dir, 'val', 'images')

    label_train_dir = os.path.join(source_dir, 'labels', 'train')
    if not os.path.exists(label_train_dir):
        label_train_dir = os.path.join(source_dir, 'train', 'labels')

    label_val_dir = os.path.join(source_dir, 'labels', 'val')
    if not os.path.exists(label_val_dir):
        label_val_dir = os.path.join(source_dir, 'val', 'labels')

    # Make sure we found the directories
    for dir_path in [img_train_dir, img_val_dir, label_train_dir, label_val_dir]:
        if not os.path.exists(dir_path):
            print(f"Warning: Directory not found: {dir_path}")
            continue

    # Copy original files to output directory first
    for split in ['train', 'val']:
        img_src_dir = img_train_dir if split == 'train' else img_val_dir
        label_src_dir = label_train_dir if split == 'train' else label_val_dir

        img_dst_dir = os.path.join(output_dir, 'images', split)
        label_dst_dir = os.path.join(output_dir, 'labels', split)

        if os.path.exists(img_src_dir):
            print(f"Copying original {split} images...")
            for img_file in tqdm(os.listdir(img_src_dir)):
                if img_file.endswith(('.jpg', '.jpeg', '.png')):
                    shutil.copy2(
                        os.path.join(img_src_dir, img_file),
                        os.path.join(img_dst_dir, img_file)
                    )

        if os.path.exists(label_src_dir):
            print(f"Copying original {split} labels...")
            for label_file in tqdm(os.listdir(label_src_dir)):
                if label_file.endswith('.txt'):
                    shutil.copy2(
                        os.path.join(label_src_dir, label_file),
                        os.path.join(label_dst_dir, label_file)
                    )

    # Augment only the training set
    augment_types = ['flip', 'brightness', 'contrast', 'rotate']

    # Get list of training images
    train_img_dir = os.path.join(output_dir, 'images', 'train')
    train_img_files = [f for f in os.listdir(train_img_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]

    print(f"Augmenting training set with factor {factor}...")
    for i in range(factor - 1):  # -1 because we already copied the original files
        for img_file in tqdm(train_img_files):
            # Choose a random augmentation type
            aug_type = random.choice(augment_types)

            # Get corresponding label file
            base_name = os.path.splitext(img_file)[0]
            label_file = f"{base_name}.txt"
            label_path = os.path.join(output_dir, 'labels', 'train', label_file)

            # Check if label file exists
            if not os.path.exists(label_path):
                continue

            # Read image
            img_path = os.path.join(train_img_dir, img_file)
            img = cv2.imread(img_path)
            if img is None:
                continue

            # Read bounding boxes
            bboxes = []
            with open(label_path, 'r') as f:
                for line in f:
                    parts = line.strip().split()
                    if len(parts) >= 5:
                        cls_id = int(parts[0])
                        x_center = float(parts[1])
                        y_center = float(parts[2])
                        width = float(parts[3])
                        height = float(parts[4])
                        bboxes.append([cls_id, x_center, y_center, width, height])

            # Apply augmentation
            aug_img, aug_bboxes = augment_image(img, bboxes, aug_type)

            # Save augmented image with a new name
            aug_img_name = f"{base_name}_{aug_type}_{i}.jpg"
            aug_img_path = os.path.join(train_img_dir, aug_img_name)
            cv2.imwrite(aug_img_path, aug_img)

            # Save augmented labels
            aug_label_name = f"{base_name}_{aug_type}_{i}.txt"
            aug_label_path = os.path.join(output_dir, 'labels', 'train', aug_label_name)
            with open(aug_label_path, 'w') as f:
                for box in aug_bboxes:
                    f.write(f"{box[0]} {box[1]} {box[2]} {box[3]} {box[4]}\n")

    # Update data.yaml
    # Count classes from label files if not specified in original yaml
    if data_yaml['nc'] == 0:
        classes = set()
        label_files = os.listdir(os.path.join(output_dir, 'labels', 'train'))
        for label_file in label_files:
            if label_file.endswith('.txt'):
                with open(os.path.join(output_dir, 'labels', 'train', label_file), 'r') as f:
                    for line in f:
                        parts = line.strip().split()
                        if len(parts) >= 5:
                            classes.add(int(parts[0]))
        data_yaml['nc'] = len(classes)

    # Update paths in data.yaml
    data_yaml['path'] = os.path.abspath(output_dir)
    data_yaml['train'] = './images/train'
    data_yaml['val'] = './images/val'

    # Save updated data.yaml
    with open(os.path.join(output_dir, 'data.yaml'), 'w') as f:
        yaml.dump(data_yaml, f, default_flow_style=None)

    print(f"Dataset augmentation completed. Augmented dataset saved to: {output_dir}")
    print(f"Original dataset size: {len(train_img_files)} training images")
    print(f"Augmented dataset size: {len(os.listdir(train_img_dir))} training images")

def main():
    parser = argparse.ArgumentParser(description="Simple dataset augmentation for YOLOv8")
    parser.add_argument("--source", type=str, required=True, help="Source dataset directory")
    parser.add_argument("--output", type=str, required=True, help="Output directory for augmented dataset")
    parser.add_argument("--factor", type=int, default=2, help="Augmentation factor (how many times to augment)")

    args = parser.parse_args()
    augment_dataset(args.source, args.output, args.factor)

if __name__ == "__main__":
    main()
EOL

                    # Run the augmentation script
                    echo "Running dataset augmentation..."
                    python ${KD_REPO}/simple_augmentation.py --source "${DATASET_DIR}" --output "${AUGMENTATION_DIR}" --factor 2

                    # Save the augmented dataset path
                    echo "${AUGMENTATION_DIR}" > ${AUGMENTATION_PATH_FILE}
                    echo "Augmentation directory path saved: ${AUGMENTATION_DIR}"

                    # Copy the augmented data.yaml to artifacts
                    cp -f ${AUGMENTATION_DIR}/data.yaml ${ARTIFACT_DIR}/augmented_data.yaml

                    # Create a sample zip for artifacts
                    echo "Creating zip file of augmented dataset sample..."
                    cd "${AUGMENTATION_DIR}"
                    mkdir -p sample
                    find images/train -name "*.jpg" | head -n 10 | xargs -I{} cp --parents {} sample/
                    find labels/train -name "*.txt" | head -n 10 | xargs -I{} cp --parents {} sample/
                    cp -f data.yaml sample/
                    cd sample
                    zip -r augmented_dataset_sample.zip ./*
                    cp augmented_dataset_sample.zip ${WORKSPACE}/email_artifacts/

                    echo "Dataset augmentation completed."
                '''

                // Archive artifacts
                archiveArtifacts artifacts: 'augmentation_dir_path.txt', fingerprint: true
                archiveArtifacts artifacts: 'artifacts/augmented_data.yaml', fingerprint: true
                archiveArtifacts artifacts: 'email_artifacts/augmented_dataset_sample.zip', fingerprint: true
            }
        }

        stage('Knowledge Distillation Parameter Study') {
            steps {
                sh '''#!/bin/bash -e
                    # Activate virtual environment
                    source ${VENV_NAME}/bin/activate

                    # Get dataset path (augmented or original)
                    if [ -f "${AUGMENTATION_PATH_FILE}" ]; then
                        DATASET_DIR=$(cat ${AUGMENTATION_PATH_FILE})
                        YAML_PATH="${DATASET_DIR}/data.yaml"
                    else
                        # Fallback to original dataset
                        DATA_YAML_PATH=$(cat downloads/data_yaml_path.txt)
                        YAML_PATH="${DATA_YAML_PATH}"
                    fi

                    echo "Using dataset YAML: ${YAML_PATH}"

                    # Create directory for knowledge distillation results
                    mkdir -p ${KD_RESULT_DIR}

                    # Log start time for performance tracking
                    echo "KD Start Time: $(date)" > ${ARTIFACT_DIR}/kd_timing.txt

                    # Define different alpha and temperature combinations to try
                    ALPHAS=(0.3 0.5 0.7)
                    TEMPERATURES=(1.0 2.0 3.0)

                    echo "Training multiple student models with different alpha and temperature combinations..."

                    # Create results tracking file
                    RESULTS_FILE="${KD_RESULT_DIR}/parameter_comparison.csv"
                    echo "alpha,temperature,map50,map50_95,model_size_mb,path" > $RESULTS_FILE

                    # Download pre-trained teacher model
                    echo "Downloading pre-trained teacher model: ${params.TEACHER_MODEL}.pt"
                    python -c "from ultralytics import YOLO; YOLO('${params.TEACHER_MODEL}.pt')"
                    TEACHER_PATH=$(find ~/.cache -name "${params.TEACHER_MODEL}.pt" | head -1)

                    if [ -z "$TEACHER_PATH" ]; then
                        echo "Teacher model not found in cache, downloading..."
                        mkdir -p ${KD_RESULT_DIR}/models
                        python -c "
from ultralytics import YOLO
model = YOLO('${params.TEACHER_MODEL}.pt')
model.save('${KD_RESULT_DIR}/models/${params.TEACHER_MODEL}.pt')
"
                        TEACHER_PATH="${KD_RESULT_DIR}/models/${params.TEACHER_MODEL}.pt"
                    fi

                    echo "Using teacher model: ${TEACHER_PATH}"

                    # Train multiple models with different parameter combinations
                    for ALPHA in "${ALPHAS[@]}"; do
                        for TEMP in "${TEMPERATURES[@]}"; do
                            echo "=================================================================="
                            echo "Training student model with alpha=${ALPHA}, temperature=${TEMP}"
                            echo "=================================================================="

                            # Set the model name with parameters in it
                            STUDENT_NAME="yolov8_kd_a${ALPHA}_t${TEMP}_${BUILD_NUMBER}"
                            STUDENT_NAME_SAFE=$(echo $STUDENT_NAME | tr '.' '_')

                            # Run knowledge distillation with this parameter combination
                            set +e  # Don't exit on error
                            python ${KD_REPO}/knowledge_distillation.py \
                                --data "${YAML_PATH}" \
                                --teacher_model "${TEACHER_PATH}" \
                                --student_model "${params.STUDENT_MODEL}.yaml" \
                                --epochs ${params.KD_EPOCHS} \
                                --batch 16 \
                                --imgsz 640 \
                                --student_name "${STUDENT_NAME_SAFE}" \
                                --alpha ${ALPHA} \
                                --temperature ${TEMP}
                            KD_STATUS=$?
                            set -e  # Restore exit on error

                            if [ $KD_STATUS -ne 0 ]; then
                                echo "Warning: Knowledge distillation for alpha=${ALPHA}, temperature=${TEMP} failed with status ${KD_STATUS}. Continuing with next combination."
                                continue
                            fi

                            # Find the student model path
                            STUDENT_MODEL_PATH=$(find ${WORKSPACE}/runs/detect -name "best.pt" | grep "${STUDENT_NAME_SAFE}" | head -1)

                            if [ ! -f "$STUDENT_MODEL_PATH" ]; then
                                echo "Warning: Couldn't find model for alpha=${ALPHA}, temperature=${TEMP}"
                                continue
                            fi

                            # Create a copy of the student model with parameters in the name
                            mkdir -p ${KD_RESULT_DIR}/models
                            MODEL_COPY="${KD_RESULT_DIR}/models/student_a${ALPHA}_t${TEMP}.pt"
                            cp "${STUDENT_MODEL_PATH}" "${MODEL_COPY}"

                            # Evaluate the student model
                            echo "Evaluating student model with alpha=${ALPHA}, temperature=${TEMP}..."
                            python -c "
from ultralytics import YOLO
import json
import os

model = YOLO('${STUDENT_MODEL_PATH}')
results = model.val(data='${YAML_PATH}')

metrics = {
    'map50': float(results.box.map50),
    'map50_95': float(results.box.map),
    'model_size_mb': os.path.getsize('${STUDENT_MODEL_PATH}') / (1024 * 1024)
}

with open('${KD_RESULT_DIR}/metrics_a${ALPHA}_t${TEMP}.json', 'w') as f:
    json.dump(metrics, f, indent=2)

print(f'{ALPHA},{TEMP},{metrics[\"map50\"]},{metrics[\"map50_95\"]},{metrics[\"model_size_mb\"]},{MODEL_COPY}')
" >> $RESULTS_FILE

                            echo "Completed training and evaluation for alpha=${ALPHA}, temperature=${TEMP}"
                        done
                    done

                    # Log end time
                    echo "KD End Time: $(date)" >> ${ARTIFACT_DIR}/kd_timing.txt

                    # Analyze results to find best model
                    echo "Analyzing results to find best model..."
                    python ${KD_REPO}/analyze_results.py --results "${RESULTS_FILE}" --output "${KD_RESULT_DIR}/analysis"

                    # Copy best model
                    if [ -f "${KD_RESULT_DIR}/analysis/best_model.txt" ]; then
                        BEST_MODEL_PATH=$(grep "Path:" ${KD_RESULT_DIR}/analysis/best_model.txt | cut -d' ' -f2)
                        if [ -f "$BEST_MODEL_PATH" ]; then
                            echo "Copying best model to ${KD_RESULT_DIR}/${params.MODEL_NAME}.pt"
                            cp "$BEST_MODEL_PATH" "${KD_RESULT_DIR}/${params.MODEL_NAME}.pt"
                        fi
                    fi

                    # Create a visualization of alpha and temperature impact
                    echo "Creating visualizations of parameter impact..."
                    cp ${KD_RESULT_DIR}/analysis/*.png ${WORKSPACE}/email_artifacts/
                    cp ${KD_RESULT_DIR}/analysis/parameter_study_report.md ${WORKSPACE}/email_artifacts/

                    echo "Knowledge distillation parameter study completed successfully."
                '''

                // Archive results and artifacts
                archiveArtifacts artifacts: 'kd_results/**', fingerprint: true, allowEmptyArchive: true
                archiveArtifacts artifacts: 'artifacts/kd_timing.txt', fingerprint: true, allowEmptyArchive: true
                archiveArtifacts artifacts: 'email_artifacts/*.png', fingerprint: true, allowEmptyArchive: true
                archiveArtifacts artifacts: 'email_artifacts/*.md', fingerprint: true, allowEmptyArchive: true
            }
        }

        stage('Model Export and Optimization') {
            steps {
                sh '''#!/bin/bash -e
                    # Activate virtual environment
                    source ${VENV_NAME}/bin/activate

                    # Find the best model
                    BEST_MODEL_PATH="${KD_RESULT_DIR}/${params.MODEL_NAME}.pt"

                    if [ ! -f "$BEST_MODEL_PATH" ]; then
                        echo "Best model not found at $BEST_MODEL_PATH, looking for alternatives..."
                        BEST_MODEL_PATH=$(find ${KD_RESULT_DIR}/models -name "*.pt" | head -1)

                        if [ ! -f "$BEST_MODEL_PATH" ]; then
                            echo "No model found. Cannot continue with export."
                            exit 1
                        fi
                    fi

                    echo "Exporting best model: $BEST_MODEL_PATH"

                    # Create export directory
                    mkdir -p ${KD_RESULT_DIR}/exports

                    # Export to different formats (ONNX, TensorRT, etc.)
                    echo "Exporting to ONNX..."
                    python -c "
from ultralytics import YOLO

model = YOLO('${BEST_MODEL_PATH}')

# Export to ONNX
model.export(format='onnx', imgsz=640)
"

                    # Find and move exported models
                    find $(dirname ${BEST_MODEL_PATH}) -name "*.onnx" -exec cp {} ${KD_RESULT_DIR}/exports/ \;

                    echo "Model export completed."

                    # Create a simple benchmark script
                    cat > ${KD_REPO}/benchmark.py << 'EOL'
from ultralytics import YOLO
import time
import argparse
import numpy as np
import json
import os

def benchmark_model(model_path, img_size=640, num_runs=100):
    """Benchmark model inference time"""
    print(f"Benchmarking {model_path}...")

    # Load model
    model = YOLO(model_path)

    # Create dummy input (black image)
    dummy_img = np.zeros((img_size, img_size, 3), dtype=np.uint8)

    # Warmup
    print("Warming up...")
    for _ in range(10):
        model(dummy_img)

    # Benchmark
    print(f"Running benchmark with {num_runs} iterations...")
    inference_times = []
    for _ in range(num_runs):
        start_time = time.time()
        model(dummy_img)
        end_time = time.time()
        inference_times.append(end_time - start_time)

    # Calculate statistics
    mean_time = np.mean(inference_times)
    median_time = np.median(inference_times)
    std_time = np.std(inference_times)
    fps = 1.0 / mean_time

    # Get model size
    model_size_mb = os.path.getsize(model_path) / (1024 * 1024)

    # Create benchmark results
    results = {
        "model_path": model_path,
        "img_size": img_size,
        "num_runs": num_runs,
        "mean_inference_time": mean_time,
        "median_inference_time": median_time,
        "std_inference_time": std_time,
        "fps": fps,
        "model_size_mb": model_size_mb
    }

    print(f"Benchmark results:")
    print(f"  Mean inference time: {mean_time:.4f} seconds")
    print(f"  Median inference time: {median_time:.4f} seconds")
    print(f"  FPS: {fps:.2f}")
    print(f"  Model size: {model_size_mb:.2f} MB")

    return results

def main():
    parser = argparse.ArgumentParser(description="Benchmark YOLOv8 model inference")
    parser.add_argument("--model", type=str, required=True, help="Path to model file")
    parser.add_argument("--img-size", type=int, default=640, help="Image size")
    parser.add_argument("--num-runs", type=int, default=100, help="Number of benchmark runs")
    parser.add_argument("--output", type=str, help="Path to save benchmark results as JSON")

    args = parser.parse_args()
    results = benchmark_model(args.model, args.img_size, args.num_runs)

    if args.output:
        with open(args.output, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"Benchmark results saved to {args.output}")

if __name__ == "__main__":
    main()
EOL

                    # Run benchmark on the exported model
                    echo "Running benchmark on the exported model..."
                    python ${KD_REPO}/benchmark.py --model ${BEST_MODEL_PATH} --num-runs 50 --output ${KD_RESULT_DIR}/benchmark_results.json

                    # Copy benchmark results to email artifacts
                    cp ${KD_RESULT_DIR}/benchmark_results.json ${WORKSPACE}/email_artifacts/
                '''

                // Archive model exports
                archiveArtifacts artifacts: 'kd_results/exports/**', fingerprint: true, allowEmptyArchive: true
                archiveArtifacts artifacts: 'kd_results/benchmark_results.json', fingerprint: true, allowEmptyArchive: true
                archiveArtifacts artifacts: 'email_artifacts/benchmark_results.json', fingerprint: true, allowEmptyArchive: true
            }
        }

        stage('Generate Final Report') {
            steps {
                sh '''#!/bin/bash -e
                    # Activate virtual environment
                    source ${VENV_NAME}/bin/activate

                    # Create a directory for the final report
                    mkdir -p ${ARTIFACT_DIR}/final_report

                    # Create a simple HTML report
                    cat > ${ARTIFACT_DIR}/final_report/kd_report.html << 'ENDOFHTML'
<!DOCTYPE html>
<html>
<head>
    <title>YOLOv8 Knowledge Distillation Report</title>
    <style>
        body { font-family: Arial, sans-serif; line-height: 1.6; margin: 20px; }
        h1, h2, h3 { color: #333; }
        .container { max-width: 1200px; margin: 0 auto; }
        .section { border: 1px solid #ddd; padding: 15px; margin-bottom: 20px; border-radius: 4px; }
        pre { background: #f8f8f8; padding: 10px; overflow-x: auto; }
        table { border-collapse: collapse; width: 100%; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #f2f2f2; }
    </style>
</head>
<body>
    <div class="container">
        <h1>YOLOv8 Knowledge Distillation Report</h1>
        <p><strong>Build #BUILD_NUMBER</strong> - Generated on GENERATION_DATE</p>

        <div class="section">
            <h2>Model Information</h2>
            <table>
                <tr><th>Parameter</th><th>Value</th></tr>
                <tr><td>Model Name</td><td>MODEL_NAME</td></tr>
                <tr><td>Teacher Model</td><td>TEACHER_MODEL</td></tr>
                <tr><td>Student Model</td><td>STUDENT_MODEL</td></tr>
                <tr><td>KD Epochs</td><td>KD_EPOCHS</td></tr>
            </table>
        </div>

        <div class="section">
            <h2>Knowledge Distillation Parameter Study</h2>
            <p>Multiple combinations of alpha and temperature values were tested to find the optimal settings.</p>

            <h3>Best Model Parameters</h3>
            <pre>BEST_MODEL_INFO</pre>

            <h3>Benchmark Results</h3>
            <pre>BENCHMARK_RESULTS</pre>
        </div>

        <div class="section">
            <h2>Build Timing Information</h2>
            <pre>KD_TIMING</pre>
        </div>

        <div class="section">
            <h2>Next Steps</h2>
            <p>The optimized model has been saved and can be used for inference in your applications.</p>
            <p>The model is available in the following formats:</p>
            <ul>
                <li>PyTorch (.pt)</li>
                <li>ONNX (.onnx)</li>
            </ul>
        </div>
    </div>
</body>
</html>
ENDOFHTML

                    # Replace placeholders with actual content
                    sed -i "s|BUILD_NUMBER|${BUILD_NUMBER}|g" ${ARTIFACT_DIR}/final_report/kd_report.html
                    sed -i "s|GENERATION_DATE|$(date)|g" ${ARTIFACT_DIR}/final_report/kd_report.html
                    sed -i "s|MODEL_NAME|${params.MODEL_NAME}|g" ${ARTIFACT_DIR}/final_report/kd_report.html
                    sed -i "s|TEACHER_MODEL|${params.TEACHER_MODEL}|g" ${ARTIFACT_DIR}/final_report/kd_report.html
                    sed -i "s|STUDENT_MODEL|${params.STUDENT_MODEL}|g" ${ARTIFACT_DIR}/final_report/kd_report.html
                    sed -i "s|KD_EPOCHS|${params.KD_EPOCHS}|g" ${ARTIFACT_DIR}/final_report/kd_report.html

                    # Get best model info
                    BEST_MODEL_INFO=$(cat ${KD_RESULT_DIR}/analysis/best_model.txt 2>/dev/null || echo "No best model information available")
                    # Get benchmark results
                    BENCHMARK_RESULTS=$(cat ${KD_RESULT_DIR}/benchmark_results.json 2>/dev/null || echo "No benchmark results available")
                    # Get KD timing
                    KD_TIMING=$(cat ${ARTIFACT_DIR}/kd_timing.txt 2>/dev/null || echo "No knowledge distillation timing data available")

                    # Create temporary files with the content
                    echo "${BEST_MODEL_INFO}" > /tmp/best_model_info.txt
                    echo "${BENCHMARK_RESULTS}" > /tmp/benchmark_results.txt
                    echo "${KD_TIMING}" > /tmp/kd_timing.txt

                    # Use perl to do multiline replacements
                    perl -i -0pe "s|BEST_MODEL_INFO|$(cat /tmp/best_model_info.txt | sed 's/[\\&/]/\\\\&/g')|g" ${ARTIFACT_DIR}/final_report/kd_report.html
                    perl -i -0pe "s|BENCHMARK_RESULTS|$(cat /tmp/benchmark_results.txt | sed 's/[\\&/]/\\\\&/g')|g" ${ARTIFACT_DIR}/final_report/kd_report.html
                    perl -i -0pe "s|KD_TIMING|$(cat /tmp/kd_timing.txt | sed 's/[\\&/]/\\\\&/g')|g" ${ARTIFACT_DIR}/final_report/kd_report.html

                    # Copy the report to email artifacts
                    cp ${ARTIFACT_DIR}/final_report/kd_report.html ${WORKSPACE}/email_artifacts/

                    echo "Build report generated successfully."

                    # Clean up temporary files
                    rm -f /tmp/best_model_info.txt /tmp/benchmark_results.txt /tmp/kd_timing.txt
                '''

                // Archive the final report
                archiveArtifacts artifacts: 'artifacts/final_report/**', fingerprint: true, allowEmptyArchive: true
                archiveArtifacts artifacts: 'email_artifacts/kd_report.html', fingerprint: true, allowEmptyArchive: true
            }
        }
    }

    post {
        success {
            echo "Pipeline completed successfully. YOLOv8 knowledge distillation completed."
            emailext (
                subject: "SUCCESS: YOLOv8 Knowledge Distillation Pipeline '${currentBuild.fullDisplayName}'",
                body: """
                    <html>
                    <body>
                        <h2>YOLOv8 Knowledge Distillation Pipeline Completed Successfully!</h2>

                        <p><strong>Job:</strong> ${env.JOB_NAME}<br>
                        <strong>Build Number:</strong> ${env.BUILD_NUMBER}<br>
                        <strong>Build URL:</strong> <a href="${env.BUILD_URL}">${env.BUILD_URL}</a></p>

                        <h3>Model Information:</h3>
                        <ul>
                            <li><strong>Model Name:</strong> ${params.MODEL_NAME}</li>
                            <li><strong>Teacher Model:</strong> ${params.TEACHER_MODEL}</li>
                            <li><strong>Student Model:</strong> ${params.STUDENT_MODEL}</li>
                            <li><strong>KD Epochs:</strong> ${params.KD_EPOCHS}</li>
                        </ul>

                        <p>The pipeline executed the following steps:</p>
                        <ol>
                            <li>Downloaded and prepared the dataset</li>
                            <li>Performed dataset augmentation</li>
                            <li>Conducted knowledge distillation parameter study</li>
                            <li>Created optimized model</li>
                        </ol>

                        <p>See the attached reports for more details on the model's performance.</p>
                    </body>
                    </html>
                """,
                mimeType: 'text/html',
                attachLog: true,
                attachmentsPattern: 'email_artifacts/*.html,email_artifacts/*.md,email_artifacts/*.png,email_artifacts/*.json',
                to: "youremail@example.com"
            )
        }

        failure {
            echo "Pipeline failed. Check the logs for details."
            emailext (
                subject: "FAILURE: YOLOv8 Knowledge Distillation Pipeline '${currentBuild.fullDisplayName}'",
                body: """
                    <html>
                    <body>
                        <h2>YOLOv8 Knowledge Distillation Pipeline Failed</h2>

                        <p><strong>Job:</strong> ${env.JOB_NAME}<br>
                        <strong>Build Number:</strong> ${env.BUILD_NUMBER}<br>
                        <strong>Build URL:</strong> <a href="${env.BUILD_URL}">${env.BUILD_URL}</a></p>

                        <p>Please check the attached log for details on the failure.</p>
                    </body>
                    </html>
                """,
                mimeType: 'text/html',
                attachLog: true,
                to: "youremail@example.com"
            )
        }

        cleanup {
            echo "Cleaning up workspace..."

            script {
                if (params.CLEAN_WORKSPACE) {
                    // Clean up large files but keep important artifacts
                    sh '''
                        # Remove the dataset zip file
                        rm -f downloads/${OUTPUT_ZIP}

                        # Only remove the virtual environment if explicitly requested
                        rm -rf ${VENV_NAME}

                        # Remove large intermediate files
                        find ${WORKSPACE} -name "*.pt" -size +100M -delete

                        # Keep important logs and results
                        echo "Cleanup completed. Artifacts preserved."
                    '''
                } else {
                    echo "Workspace cleanup skipped as per user request."
                }
            }
        }
    }
}